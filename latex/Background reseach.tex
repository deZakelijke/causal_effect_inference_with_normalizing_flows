\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usepackage{standalone}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[a4paper, total={6in, 9in}]{geometry}



\newcommand{\Norm}{\mathcal{N}}
\newcommand{\Loss}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bU}{\mathbb{U}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bz}{\mathbf{z}}

\newcommand{\parfrac}[2]{\frac{\partial #1}{\partial#2}}

\title{Causal Effect Inference using Normalizing Flows}
\author{Micha de Groot}
\date{August 2019}

\begin{document}


\section{Background research}

\subsection*{Do calculus and interventions?}



\subsection*{Causal Effect Inference with Deep Latent-Variable models}
The paper by Louizos et al.\cite{louizos2017causal} serves as a good starting point for research into the use of Normalising Flows in causal inference. It explores the possibility of measuring the direct effect of one observed variable on another given that there is a shared unobserved confounder by using a proxy variable that has the same confounder. This causal graph is shown in Figure \ref{fig:causal_graph}. Their contribution is doing this task by using Variational Autoencoders\cite{kingma2013auto} to recover the Individual Treatment Effect(ITE):
\begin{equation}\label{equation:ITE}
    ITE(x) := \mathbb{E}[\by | \bX = x, do(\bt = 1)] - \mathbb{E}[\by | \bX = x, do(\bt=0)]
\end{equation}

\noindent
The paper states that $\bt$ does not have to be binary for their approach to work but it is assumed for simplicity and compatibility with prior benchmarks. First the joint distribution $p(\bZ, \bX, \by, \bt)$ is approximated with a VAE that consists of several neural networks. With that $p(\by | \bX, do(\bt=1))$ can be identified:

\begin{equation}\label{equation:intervention}
    \begin{split}
    p(\by | \bX, do(\bt=1)) &= \int_\bZ p(\by | \bX, do(\bt=1, \bZ)) p(\bZ | \bX, do(\bt = 1)) d\bZ \\
                            &= \int_\bZ p(\by | \bX, \bt = 1, \bZ) p(\bZ | \bX) d\bZ
    \end{split}
\end{equation}

\noindent
The inference network captures $q(\by | \bt , \bX), q(\bZ | \bt, \by, \bX), q(\bZ| \bt, \by, \bX)$ and the generative network captures $p(\bX | \bZ), p(\bt | \bZ), p(\by | \bt, \bZ)$. These are then used to compute a sample average for the ITE.

The problem with trying to translate this to Normalising-Flow model is that an NF model cannot simply model conditional distributions. For this we would require a way to model arbirary Bayesian networks with NF. 

\subsection*{Analyzing Inverse Problems with Invertible Neural Networks}
The invertibility of flow-based models makes them suitable for recovering posterior information of complex physical systems. This has been proposed by Ardizzone et al. \cite{ardizzone2018analyzing} in the form of Invertible Neural Networks. Given some process $\bx \rightarrow \by$ with observed $\by$ and unobserved $\bx$ oe would like to know how to recover $\bx$ for a ceartain $\by$. This is done by using an approach based on the RealNVP\cite{dinh2016density} model. Since the forward process has inherent information loss, a latent variable $\bz$ is introduced that captures the information in $\bx$ that is not contained in $\by$, turning the process into $f(\bx) = [\by, \bz]$  and $\bx = f^{-1}(\by, \bz) = g(\by, \bz)$. Because of the nature of Normalising-Flow models, $g(\by, \bz)$ does not have to be modelled separately. The density $p(\bz)$ is shaped as a Gaussian. 

This differs from the causal graph in Figure \ref{fig:causal_graph} because both $\bx$ and $\bz$ are unobserved. Though the INN approach might still be useful if we were to recover $\bZ$ from a given $\bX$. I think this would be an approximation for $p(\bZ | \bX)$, which could then be used to approximate equation \ref{equation:intervention}. 

\begin{figure}
    \centering
    \includestandalone{Figures/causal_graph_one_proxy_one_confounder}
    \caption{Causal graph with one proxy variable. $\bt$ is the variable we intervene on; $\by$ is the outcome; $\bZ$ is the latent confounder; and $\bX$ is the observed proxy variable.}
    \label{fig:causal_graph}
\end{figure}


\subsection*{Variational Inference with Normalising Flows}
A normalising flows is a type of model that is capable of specifying flexible, arbitrarily complex and scalable approximate posterior distributions in variational inference\cite{rezende2016variational}. The idea is to start with a simple initial density and transforming that into an arbitrary complex distribution by applying a series of invertible transformations. The basis lies in the original negative free energy function derived from introducing the approximate posterior distribution in the marginal log likelihood:

\begin{equation}\label{equation:neg_free_energy}
    \begin{split}
    \log p_\theta(\bx) &= \log \int p_\theta(\bx | \bz) p(\bz) d\bz\\
    &= \log \int \frac{q_\phi(\bz|\bx)}{q_\phi(\bz|\bx)} p_\theta(\bx | \bz) p(\bz) d\bz\\
    &\geq -\mathbb{D}_{KL}[q_\phi(\bz|\bx) || p(\bz)] + \mathbb{E}_q[\log p_\theta(\bx|\bz)] = -\mathcal{F}(\bx)
    \end{split}    
\end{equation}
The negative free energy function $\mathcal{F}$ is also known as the Evidence Lower Bound(ELBO). The novelty of normalising flows lies in how the approximate posterior $q_\phi(\bz|\bx)$ is defined.  

\subsection*{Distinguishing cause from effect using observational data: methods and benchmarks}
--Book (chapter)\cite{mooij2016distinguishing}, quite long, will look for useful info--

Causal discovery in the bivariate case has been mostly based on the difference in complexity of the factorisation of the joint distribution $p(X,Y)$. If $p(X)p(Y|X)$ consists of two simpler distributions than $p(Y)p(X|Y)$ then it is likely that $X$ is a cause of $Y$ instead of the other way around. The chapter discusses several extensive benchmarks for these types of tests.

Another definition for "$X$ is a cause of $Y$" is that $p(Y|do(X=x)) \neq p(Y|do(X=x'))$

\subsection*{Density estimation using Real-NVP}

\subsection*{Sylvester Normalising Flows for Variational Inference}


\section{Possible research directions}
\subsubsection*{More complex graphs}
In the reviews of \cite{louizos2017causal} it is mentioned that it might be interesting to look at more complex causal graphs with more proxy variables such as those discussed in \cite{miao2018identifying}. From what I gathered these other setups with confounders are more specific instances of the problem and the graph in figure \ref{fig:causal_graph} is the most general case. 

\subsubsection*{Causal graph discovery with NF}
Complex problem. Solved for small number of variables. In the simplest form we are examining if $p(x) = p(x|y)$ or $p(x) \neq p(x|y)$, which requires the modelling of conditional distributions.

\subsubsection*{Improve on CEVAE with NF}
Requires some way to mode conditional probabilities with NF. Has the possibility of increasing the lower bound.

\section*{Possible designs of Normalising Flow}
For now we focus on 
\subsection{}
\begin{equation}
    p(\by | \bx, \bt) = \frac{p(\by, \bx, \bt)}{\sum_\by p(\by, \bx, \bt)}
\end{equation}
\begin{equation}
    p(\by, \bx, \bt) = p(\bz) |J|
\end{equation}
\begin{equation}
    p(\by | \bx, \bt) = \frac{p(\bz)|J|}{\sum_\by p(\bz)|J|}
\end{equation}
Simplest version. Probably not strong enough model. We can compute $\bz$ from the joint distribution of the other variables, but it isn't straightforward to me how we would sum over $\by$.

\subsection{}
Splitting $\bz$ into multiple variables might allow :
\begin{align}
    p(\bx, \bt, \by) &= p(\bx)p(\by|\bt)p(\bt)\\
    p(\bx) &= p(\bz_x) |J|\\
    p(\bt) &= p(\bz_t) |J|\\
    p(\by| \bt) &= p(\bz_y|\bt) |J|\\
\end{align}
or
\begin{align}
    p(\bt, \by | \bx) &= p(\bt) p(\by | \bt, \bx)\\
    p(\bt) &= p(\bz_t) |J|\\
    p(\by | \bt, \bx) &= p(\bz_y) |J|
\end{align}
For either of these two to make sense there would have to be a common latent confounder for all $\bz$ variables, called $\bU$, which could be recovered from the $\bz$ variables in the same way that is done by Ardizzone et al. \cite{ardizzone2018analyzing}. We could then sample from $p(\bU)$, invert the flow to get $\bz_t$ and $\bz_x$. By then conditioning on $\bt$, $\bz_y$ can be obtained and form that $\by$. These two options are represented in Figure \ref{fig:causal_graph_split_confounder}.

Problem here is that during training the "forward process", as it is called by \cite{ardizzone2018analyzing}, is known and can be used to sample an arbitrary amount of training data by sampling from the appropriate prior. Since we have no information on the real confounder, this is not really an option.

\begin{figure}
    \centering
    \includestandalone{Figures/causal_graph_one_proxy_split_confounder}
    \caption{Splitting the common confounder into multiple variables that have one shared confounder. The effect from $\bt$ can be passed through the new latent variable $\bz_y$ or not.}
    \label{fig:causal_graph_split_confounder}
\end{figure}


We need a way to measure the effect of the confounder on $\by$ given $\bx$  and $\bt$. If we have some flow form $\bx, \by$ and $\bt$ to the confounder, we can't reverse it to get $\by$. One advantage is that $\bt$ is binary. This means we can just make two distributions, one for $\bt=1$ and one for $\bt=0$ 

What if we do a sort of Monte Carlo approach where we have a flow from x, y, t to z and sample a bunch of y values and then sum/average the z values, we can then reverse the flow to get the real y?


\newpage
\subsection{Basic equations designing the setup}
\subsubsection{Three flows}
We make $\bZ$ an $N$ dimensional vector and define three flows. One from $\bx$ to $\bZ$. $\bx$ will be $M$ dimensional and the remaining $N-M$ dimensions will come form some auxiliary variable $\bZ_x$. 
Next we have a flow from $\bZ$ to $\bt$ and $\bZ_t$. $\bt$ is one dimensional and binary, $\bZ_t$ will have $N-1$ dimensions. If we want to do an intervention, we could do that after this flow. The third flow is from $\bZ_t$ and $\bt$ to $\by$ and $\bZ_y$, where we have to define some split for the $N$ dimensions from the input. A graphical representation is given in Figure \ref{fig:flow_graph_three_flows}. The joint distribution over all variables under this model is defined as:
\begin{equation}
    p(\bx, \bt, \by, \bz) = p(\bx|\bz)p(\bt|\bz)p(\by|\bt, \bz)p(\bz)
\end{equation}


\begin{figure}
    \centering
    \includestandalone{Figures/flow_graph_3_flows}
    \caption{Graphical representation of the flow model with three flows. With this model we can intervene on $\bt$ and the reverse flow can be initiated with only $\bx$.}
    \label{fig:flow_graph_three_flows}
\end{figure}

\noindent
The transitions in Figure \ref{fig:flow_graph_three_flows} will be modelled by the following set of equations:
\begin{align}
    [\bz, \bZ_x] = f_x(\bz)\quad &(=) \quad \bz = g_x(\bx, \bZ_x)\\
    [\bt, \bz_t] = f_t(\bz)\quad &(=) \quad \bz = g_t(\bt, \bZ_t\\
    [\by, \bZ_y] = f_y(\bt, \bZ_t)\quad &(=) \quad [\bt, \bZ_t] = g_y(\by, \bZ_y)
\end{align}
\noindent
And the following set of probability distributions:
\begin{align}
    \bz \sim p(\bz) &= \Norm(\bz | 0, I)\\
    \bZ_x \sim p(\bZ_x) &= \Norm(\bZ_x | 0, I)\\
    \bZ_t \sim p(\bZ_t) &= \Norm(\bZ_t | 0, I)\\
    \bZ_z \sim p(\bZ_z) &= \Norm(\bZ_z | 0, I)\\
    q(\bx, \bZ_x) &= p(\bz) \left|\det\parfrac{f_x(\bz)}{\bz}\right|^{-1}\\
    q(\bt, \bZ_t) &= p(\bz) \left|\det\parfrac{f_t(\bz)}{\bz}\right|^{-1}\\
    q(\by, \bZ_y) &= p(\bz) \left|\det\parfrac{f_y(f_t(\bz))}{\bz}\right|^{-1}
\end{align}

\subsubsection{Two flows}
For the second proposal most of the setup is kept the same. What changes is that $\bZ_x$ and $\bZ_t$ are merged into one random variable and the normalising flow $f_x$ and $f_t$ are merged into one flow. This result in $\bz \in \R^N, \bx \in \R^M, \bt \in R, \bZ_x \in \R^{N-M-1}$ and $[\bx, \bZ_x, \bt] = f_x(\bz)$. The flow to $\by$ is kept as $[\by, \bZ_y] = f_y(\bt, \bZ_x)$. 

\begin{figure}
    \centering
    \includestandalone{Figures/flow_graph_2_flows}
    \caption{Graphical representation of the flow model with two flows. With this model the dependence between $\bx$ and $\by, \bt$ is made explicit with the first flow from $\bz$.}
    \label{fig:flow_graph_three_flows}
\end{figure}

\subsubsection{Comparison of the two model setups}
First off I think the split into at least two normalising flows is unavoidable since we have to be able to generate $\bt$ from $\bz$ before we generate $\by$. This  immediately implies that the extra latent variable $\bZ_x$ or $\bZ_x$, depending on the version, is required a well. Otherwise the information flowing from $\bz$ to $\by$ that does not go through $\bt$ is lost when inferring $\by$, starting form $\bz$. This requirement is fulfilled by both setups. 

Where they differ is in the split of the normalising flow from $\bz$ to $\bx$ and $\bt$ into two separate flows. When this is kept in one normalising flow we have the advantage that when we infer $\bz$, all our observed variables are taken into account directly. We cannot have the case that $\bz$ inferred from $\bx$ (and $\bZ_x$) is different from the inference based on $\bt, \by$ (and $\bZ_y, \bZ_t)$. The disadvantage is that it is therefore not possible to infer $\bz$ only based on $\bx$ and $\bZ_x$. We would need to generate a sample of $\bt$. In the general case this would be a problem, but in this problem $\bt$ is a binary variable and we might just infer $\bz$ for both values of $\bt$ and either average the two values of $\bz$ or use both in any downstream task.


\subsection{Loss functions and Normalising Flow design}
These equations are sufficient for modelling the intervention probability in equation \ref{equation:intervention} and to estimate the ITE in equation \ref{equation:ITE}. In broad terms there are two other components to choose/design: the type of normalising flow used to model $f_x, f_t, f_y$ and the definition of the loss function(s). We start with the loss function and what it will be composed of.

There are a lot of ways that our approximate distributions must match actual distributions, and some of these need to be incorporated in the loss:
\begin{itemize}
    \item $\Loss(q(g_x(\bx, \bZ_x)),\quad p(\bz))$
    \item $\Loss(q(g_t(\bt, \bZ_t)),\quad p(\bz))$
    \item $\Loss(q(g_t(g_y(\by, \bZ_y))),\quad p(\bz))$
    \item $\Loss(q(g_t(g_y(\by, \bZ_y))),\quad q(g_x(\bx, \bZ_x)))$
    \vspace{0.5cm}
    \item $\Loss(q(f_x(\bz)),\quad p(\bx)p(\bZ_x))$
    \item $\Loss(q(f_t(\bz)),\quad p(\bt)p(\bZ_t))$
    \item $\Loss(q(f_y(f_x(\bz))),\quad p(\by)p(\bZ_y))$
\end{itemize}

What needs some consideration is whether or not it makes sense to incorporate all these quantities into the loss function and if some supervised loss term might be required. For example, one that is not based on the probabilities of samples but on the distance between generated and real samples.



\bibliography{references.bib}
\bibliographystyle{abbrv}

\end{document}
